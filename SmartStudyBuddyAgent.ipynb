{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWG5f0Ouj5MVvH5uYnuDyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayshri/AIAgents/blob/main/SmartStudyBuddyAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement\n",
        "\n",
        "Business Problem\n",
        "\n",
        "Imagine you're a student juggling multiple subjects, each with extensive lecture notes (provided\n",
        "as text files or PDFs). It's challenging to quickly find specific information or revise concepts\n",
        "across these varied documents. You need a \"Smart Study Buddy\" – a virtual assistant that can:\n",
        "\n",
        "● Understand your questions about the course material.\n",
        "\n",
        "● Retrieve relevant information only from your uploaded lecture notes.\n",
        "\n",
        "● Maintain the context of your current study session, allowing for follow-up questions.\n",
        "This tool aims to make studying more efficient and targeted.\n",
        "\n",
        "ML Problem\n",
        "\n",
        "To build the \"Smart Study Buddy,\" we will develop a Retrieval Augmented Generation (RAG)\n",
        "system. This system will:\n",
        "\n",
        "● Data Sources (Documents): Ingest multiple local document files (e.g., .txt, .pdf) representing your lecture notes.\n",
        "\n",
        "○ Hint: You'll need to use appropriate document loaders from LangChain for\n",
        "different file types.\n",
        "\n",
        "● Vector Store: Use Pinecone to store a searchable representation (embeddings) of your\n",
        "lecture notes. This allows for efficient similarity search to find relevant context.\n",
        "\n",
        "● LLM: Leverage a Large Language Model (LLM) to understand questions, process\n",
        "retrieved context, and generate helpful answers.\n",
        "\n",
        "● Conversational Memory (In-Session): Implement a mechanism to remember the\n",
        "dialogue within the current study session, enabling coherent follow-up interactions.\n",
        "\n",
        "Project Tasks\n",
        "\n",
        "The project has been broken down into the following key tasks:\n",
        "\n",
        "1. Pinecone Setup\n",
        "\n",
        "● You will need a Pinecone account.\n",
        "\n",
        "● Initialize the Pinecone client in your notebook using your API key and environment.\n",
        "\n",
        "● Define a unique name for your Pinecone index.\n",
        "\n",
        "● Write code to create the index if it doesn't already exist. Ensure you specify the correct\n",
        "dimension for the embedding model you choose (e.g., OpenAI's\n",
        "text-embedding-ada-002 has a dimension of 1536).\n",
        "\n",
        "● Reference: Pinecone LangChain Integration\n",
        "\n",
        "\n",
        "2. Data Ingestion and Processing (Loading and Chunking)\n",
        "\n",
        "● Create 2–3 sample lecture note files (e.g., subjectA_notes.txt,\n",
        "subjectB_notes.pdf). You can fill them with dummy text or copy-paste some\n",
        "educational content.\n",
        "\n",
        "● Use appropriate LangChain document loaders to load the content from these files.\n",
        "○ TextLoader for .txt files.\n",
        "○ PyPDFLoader for .pdf files. (You might need to pip install pypdf)\n",
        "\n",
        "● Chunk the loaded documents into smaller, manageable pieces. This is crucial for fitting\n",
        "within the LLM's context window and for effective retrieval.\n",
        "○ Consider using RecursiveCharacterTextSplitter or\n",
        "CharacterTextSplitter. Experiment with chunk_size and\n",
        "chunk_overlap.\n",
        "\n",
        "○ References:\n",
        "■ CharacterTextSplitter\n",
        "■ RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "3. Embedding Generation and Vector Storage\n",
        "\n",
        "● For each document chunk, generate an embedding (a numerical vector representation).\n",
        "\n",
        "● You can use:\n",
        "○ OpenAIEmbeddings (requires an OpenAI API key).\n",
        "○ Sentence Transformer models from Hugging Face via\n",
        "HuggingFaceEmbeddings (e.g., 'all-MiniLM-L6-v2').\n",
        "\n",
        "● Store these chunks and their corresponding embeddings in your Pinecone index.\n",
        "○ Use PineconeVectorStore.from_documents() for an easy way to\n",
        "populate the index.\n",
        "○ Ensure the embedding model used here matches the dimension specified during\n",
        "Pinecone index creation.\n",
        "\n",
        "4. Conversational Question Answering Chain\n",
        "\n",
        "● Implement a chain that can answer questions based on the retrieved context and\n",
        "maintain conversation history for the current session.\n",
        "\n",
        "● ConversationalRetrievalChain is a good high-level option.\n",
        "\n",
        "● Alternatively, you can build a custom chain using a retriever from your Pinecone vector\n",
        "store and integrating a memory module.\n",
        "\n",
        "● Use a LangChain memory module like ConversationBufferMemory or\n",
        "ConversationSummaryMemory to store the chat history of the current session.\n",
        "\n",
        "● Reference: Adding Memory to QA Chains\n",
        "\n",
        "● Create a PromptTemplate to guide the LLM. The prompt should instruct the LLM to:\n",
        "○ Answer the user's question based only on the provided context (retrieved\n",
        "chunks).\n",
        "○ If the answer cannot be found in the provided context, explicitly state that the\n",
        "information is not in the lecture notes.\n",
        "○ Consider the chat history when formulating the current answer.\n",
        "○ Reference: PromptTemplate\n",
        "\n",
        "\n",
        "5. Testing and Demonstration\n",
        "\n",
        "● In your notebook, instantiate and interact with your \"Smart Study Buddy.\"\n",
        "● Test Case 1: Ask an initial question whose answer is clearly in one of your documents.\n",
        "● Test Case 2: Ask a follow-up question that relies on the context established by the\n",
        "previous question-answer pair.\n",
        "○ Example:\n",
        "■ User: \"What are the main types of RAG systems?\"\n",
        "■ Bot: \"Type A, Type B, Type C\"\n",
        "\n",
        "■ User: \"Tell me more about Type A.\"\n",
        "\n",
        "● Test Case 3: Ask a question for which the answer is not present in your documents to\n",
        "verify the \"I don't know\" or \"not in notes\" behavior.\n",
        "● Test Case 4 (Optional): Restart your notebook kernel and run the interaction part again.\n",
        "The chatbot should not remember the conversation from the previous run (as we are not\n",
        "implementing persistent user memory), but it should still be able to answer questions\n",
        "based on the documents already indexed in Pinecone.\n",
        "\n",
        "Extra Requirements (Focus on In-Session Conversation)\n",
        "The core requirement is to make the system conversational *within a single interaction session*.\n",
        "The system should understand follow-up questions that implicitly refer to previous turns in the\n",
        "current dialogue.\n",
        "\n",
        "Example Interaction:\n",
        "● User: \"What were the key topics covered in the Machine Learning lecture?\"\n",
        "● System: (Retrieves from notes) \"The key topics included Supervised Learning,\n",
        "Unsupervised Learning, and Reinforcement Learning.\"\n",
        "● User: \"Can you explain Supervised Learning in more detail based on the notes?\"\n",
        "○ (The system should understand \"Supervised Learning\" refers to the one\n",
        "mentioned in its previous response and use the notes to elaborate.)\n",
        "\n",
        "This is achieved using LangChain's memory components, ensuring the `chat_history` is passed\n",
        "appropriately."
      ],
      "metadata": {
        "id": "4851t6tvNgcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L-PecbgNcdF"
      },
      "outputs": [],
      "source": []
    }
  ]
}